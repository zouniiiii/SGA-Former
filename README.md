# SGA-Former
SGA-Former uses spectral relation metrics and a spectral-guided attention enhancer to align global self-attention with graph structural dependencies, leading to more structure-aware and expressive graph transformers

# Descreption
1. The core implementation of the Spectral-Guided Attention bias used in SAG-Former is offered in `sag_processor.py`. The full source code will be released upon paper acceptance.
2. All datasets used in our experiments are publicly available and can be directly accessed via the PyG (PyTorch Geometric) library. 
